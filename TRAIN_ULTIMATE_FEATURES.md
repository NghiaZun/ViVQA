# Training Ultimate - Multi-Task Learning vá»›i Format Enforcement

## ğŸ¯ Tá»•ng quan

PhiÃªn báº£n training tá»‘i Æ°u vá»›i **multi-task learning** Ä‘á»ƒ giáº£m overfitting vÃ  Ä‘áº£m báº£o output format Ä‘Ãºng chuáº©n XML.

## âœ¨ Features chÃ­nh

### 1. **Multi-Task Learning (3 objectives riÃªng biá»‡t)**
```python
# 3 forward passes per batch
loss_answer   = model(labels=answer_ids)      # <answer>...</answer>
loss_reasoning = model(labels=reasoning_ids)  # <reasoning>[TYPE]...</reasoning>
loss_format   = model(labels=full_format)     # Full structure

total_loss = w_answer * loss_answer + w_reasoning * loss_reasoning + w_format * loss_format
```

**Lá»£i Ã­ch:**
- âœ… Má»—i component cÃ³ signal riÃªng â†’ há»c tá»‘t hÆ¡n
- âœ… Giáº£m overfitting (separate objectives)
- âœ… Logging rÃµ rÃ ng: F, A, R Ä‘á»u cÃ³ giÃ¡ trá»‹ thá»±c

### 2. **Curriculum Learning (3 stages)**
```
Stage 1 (0-20):   Answer=50%, Format=35%, Reason=15%  â†’ Master answers first
Stage 2 (20-40):  Answer=35%, Format=35%, Reason=30%  â†’ Balance all
Stage 3 (40+):    Answer=20%, Format=20%, Reason=60%  â†’ Reasoning quality
```

### 3. **Anti-Overfitting Techniques**

#### Label Smoothing (0.1)
```python
# Thay vÃ¬: true_label=1.0, others=0.0
# DÃ¹ng: true_label=0.9, others=0.1/(vocab-1)
```
â†’ Model khÃ´ng quÃ¡ tá»± tin, generalize tá»‘t hÆ¡n

#### Image Augmentation
```python
transforms.Compose([
    RandomHorizontalFlip(p=0.3),
    ColorJitter(brightness=0.2, contrast=0.2),
    RandomRotation(degrees=5)
])
```
â†’ TÄƒng diversity cá»§a training data

#### EMA (Exponential Moving Average)
```python
# Smooth model weights: Î¸_ema = 0.999 * Î¸_ema + 0.001 * Î¸_current
# Best model lÆ°u EMA weights â†’ stable hÆ¡n
```
â†’ Giáº£m variance, improve generalization

#### Gradient Clipping
```python
torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
```
â†’ Prevent gradient explosion

### 4. **Format Validation**

#### Validate output format
```python
validation = validate_format(text)
# Check: <answer>...</answer> vÃ  <reasoning>[TYPE]...</reasoning>
```

#### Auto-fix malformed output
```python
fixed_text = fix_format(text)
# Tá»± Ä‘á»™ng sá»­a output thiáº¿u tags hoáº·c sai format
```

#### Periodic checking during training
- Má»—i 5 epochs check format quality
- Report % outputs valid
- Warning náº¿u cÃ³ lá»—i format

## ğŸ“Š Expected Results

### TrÆ°á»›c (Single Loss)
```
Train Loss: 0.1439 (F:0.1439 | A:0.0000 | R:0.0000)  â† A, R khÃ´ng há»c
Val Loss:   0.2322 (F:0.2322 | A:0.0000 | R:0.0000)  â† Overfitting
```

### Sau (Multi-Task)
```
Train Loss: 0.1250 (F:0.0420 | A:0.0380 | R:0.0450)  â† Táº¥t cáº£ Ä‘á»u há»c
Val Loss:   0.1420 (F:0.0480 | A:0.0430 | R:0.0510)  â† Gap nhá» hÆ¡n
Format Valid: 9/10 (90%)                               â† CÃ³ thá»ƒ parse
```

## âš™ï¸ Hyperparameters

| Parameter | Value | LÃ½ do |
|-----------|-------|-------|
| LR | 5e-6 | Tháº¥p hÆ¡n cho 3x forward passes |
| Batch Size | 2 | Giáº£m Ä‘á»ƒ fit 3 models trong memory |
| Accum Steps | 4 | Effective batch = 8 |
| Label Smoothing | 0.1 | Standard cho generation tasks |
| EMA Decay | 0.999 | Smooth averaging |
| Warmup Epochs | 3 | Stabilize multi-task training |

## ğŸš€ Usage

### Training
```python
# ÄÃ£ config sáºµn, chá»‰ cáº§n cháº¡y:
python train_student_ultimate.py
```

### Inference vá»›i format validation
```python
# Generate
output = model.generate(...)
text = tokenizer.decode(output)

# Validate
validation = validate_format(text)
if not validation['valid']:
    text = fix_format(text)  # Auto-fix

# Parse
answer = validation['answer']
reasoning = validation['reasoning_text']
```

## ğŸ“ Logs & Checkpoints

### Training logs
```csv
epoch,stage,train_loss,train_F,train_R,train_A,val_loss,val_F,val_R,val_A,...
1,ANSWER_MASTERY,0.15,0.05,0.04,0.06,0.18,0.06,0.05,0.07,...
```

### Checkpoints
- `latest_checkpoint.pt` - Auto-saved má»—i epoch (resume)
- `vqa_student_best_ultimate.pt` - Best model (EMA weights)
- `vqa_student_final_ultimate.pt` - Final model (EMA weights)

## âš ï¸ Notes

1. **Training time**: ~3x cháº­m hÆ¡n (3 forward passes/batch)
2. **Memory**: ÄÃ£ optimize vá»›i batch=2, accum=4
3. **First epochs**: Loss cao do warmup (bÃ¬nh thÆ°á»ng)
4. **Format check**: Má»—i 5 epochs, Ä‘áº£m báº£o 90%+ valid

## ğŸ“ Theory

### Why Multi-Task Learning?
- Shared representations há»c tá»‘t hÆ¡n
- Regularization effect (prevent overfitting)
- Better generalization

### Why Label Smoothing?
- Prevent over-confidence
- Smoother loss landscape
- Better calibration

### Why EMA?
- Reduce variance
- More stable predictions
- Like ensemble of recent models

## ğŸ“š References

- Label Smoothing: Szegedy et al., "Rethinking the Inception Architecture" (2016)
- EMA: Polyak & Juditsky, "Acceleration of Stochastic Approximation" (1992)
- Multi-Task Learning: Caruana, "Multitask Learning" (1997)
