# KAGGLE CHECKPOINT & RESUME GUIDE

## üéØ Gi·∫£i Ph√°p Checkpoint T·ª± ƒê·ªông

### 1Ô∏è‚É£ **Auto-Checkpoint M·ªói Epoch**
Script ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·ªÉ l∆∞u checkpoint SAU M·ªñI EPOCH:
- File: `latest_checkpoint.pt`
- Ch·ª©a: model, optimizer, scheduler, epoch, best_loss
- T·ª± ƒë·ªông ghi ƒë√® ‚Üí lu√¥n c√≥ checkpoint m·ªõi nh·∫•t

### 2Ô∏è‚É£ **Backup Checkpoints**
Checkpoint ƒë·ªãnh k·ª≥ (kh√¥ng b·ªã ghi ƒë√®):
- **Ultimate**: M·ªói 10 epochs + epoch 15, 30
- **Fast**: M·ªói 5 epochs
- File: `checkpoint_epoch10.pt`, `checkpoint_epoch20.pt`...

### 3Ô∏è‚É£ **Best Model**
L∆∞u model t·ªët nh·∫•t (val_loss th·∫•p nh·∫•t):
- File: `vqa_student_best_ultimate.pt` (ho·∫∑c `_fast.pt`)
- Ch·ªâ ch·ª©a model weights (nh·∫π h∆°n)

---

## üì¶ C√°ch S·ª≠ D·ª•ng

### **Khi Session H·∫øt Gi·ªù (12h):**

1. **Download 3 files quan tr·ªçng:**
   ```python
   # Trong Kaggle notebook
   from IPython.display import FileLink
   FileLink('/kaggle/working/latest_checkpoint.pt')  # QUAN TR·ªåNG NH·∫§T
   FileLink('/kaggle/working/vqa_student_best_ultimate.pt')
   FileLink('/kaggle/working/train_val_log_ultimate.csv')
   ```

2. **Upload checkpoint l√™n Kaggle Dataset:**
   - V√†o Kaggle.com ‚Üí Datasets ‚Üí New Dataset
   - Upload `latest_checkpoint.pt`
   - T√™n dataset: `vivqa-checkpoint-epoch20` (v√≠ d·ª•)
   - Make public ho·∫∑c private

3. **Resume trong session m·ªõi:**
   ```python
   # ·ªû ƒë·∫ßu train_student_ultimate.py, s·ª≠a d√≤ng:
   RESUME_FROM = "/kaggle/input/vivqa-checkpoint-epoch20/latest_checkpoint.pt"
   
   # R·ªìi ch·∫°y l·∫°i:
   !python train_student_ultimate.py
   ```

---

## ‚ö° Script Nhanh H∆°n

### **train_student_fast.py** (M·ªöI)
T·ªëi ∆∞u cho th·ªùi gian training ng·∫Øn h∆°n:

| T√≠nh nƒÉng | Ultimate (G·ªëc) | Fast (M·ªõi) |
|-----------|---------------|-----------|
| Epochs | 100 | 30 |
| Batch size | 3 | 4 |
| Max length | 160 | 128 |
| Early stop | 15 | 8 |
| Stage 1 | 15 epochs | 5 epochs |
| Stage 2 | 30 epochs | 12 epochs |
| **Th·ªùi gian ∆∞·ªõc t√≠nh** | **~58 gi·ªù** | **~12-15 gi·ªù** |

### **C√°ch d√πng:**
```python
!python train_student_fast.py
```

**L∆∞u √Ω:** Fast mode ph√π h·ª£p cho:
- ‚úÖ Testing nhanh
- ‚úÖ Deadline g·∫•p
- ‚úÖ Ch·ªâ c·∫ßn model kh·∫£ d·ª•ng

Ultimate mode cho k·∫øt qu·∫£ t·ªët nh·∫•t (thesis).

---

## üîÑ Resume Workflow

### **Workflow t·ª± ƒë·ªông:**
```
Epoch 1-10 ‚Üí Session 1 (12h)
  ‚Üì Download latest_checkpoint.pt
  ‚Üì Upload to Kaggle Dataset
Epoch 11-20 ‚Üí Session 2 (12h) [RESUME_FROM = checkpoint]
  ‚Üì Download latest_checkpoint.pt
  ‚Üì Upload to new Dataset
Epoch 21-30 ‚Üí Session 3 (12h) [RESUME_FROM = checkpoint]
  ‚Üì ...
```

### **Script t·ª± ƒë·ªông t·∫°o checkpoint:**
```python
# SAU M·ªñI EPOCH t·ª± ƒë·ªông l∆∞u:
torch.save({
    'epoch': epoch,                          # Epoch s·ªë bao nhi√™u
    'model_state_dict': model.state_dict(),  # Tr·ªçng s·ªë model
    'optimizer_state_dict': optimizer.state_dict(),  # Optimizer state
    'scheduler_state_dict': scheduler.state_dict(),  # LR scheduler
    'best_val_loss': best_val_loss,          # Val loss t·ªët nh·∫•t
    'early_stop_counter': early_stop_counter # ƒê·∫øm early stopping
}, '/kaggle/working/latest_checkpoint.pt')
```

### **Script t·ª± ƒë·ªông resume:**
```python
if RESUME_FROM and os.path.exists(RESUME_FROM):
    checkpoint = torch.load(RESUME_FROM)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    start_epoch = checkpoint['epoch'] + 1  # Ti·∫øp t·ª•c t·ª´ epoch ti·∫øp theo
    best_val_loss = checkpoint['best_val_loss']
    print(f"‚úÖ Resuming from epoch {start_epoch}")
```

---

## üí° Tips T·ªëi ∆Øu Training

### 1. **Gi·∫£m epochs n·∫øu g·∫•p:**
```python
# Trong train_student_ultimate.py
EPOCHS = 50  # Thay v√¨ 100
```

### 2. **TƒÉng batch size n·∫øu GPU c√≤n tr·ªëng:**
```python
BATCH_SIZE = 4  # Thay v√¨ 3 (test tr∆∞·ªõc!)
```

### 3. **T·∫Øt validation th∆∞·ªùng xuy√™n:**
```python
# Validate m·ªói 2 epochs thay v√¨ m·ªói epoch
if epoch % 2 == 0:
    val_loss = validate_epoch(...)
```

### 4. **Reduce logging overhead:**
```python
# T·∫Øt ROUGE scoring trong validation (t·ªën th·ªùi gian)
# Ch·ªâ d√πng loss ƒë·ªÉ track
```

### 5. **S·ª≠ d·ª•ng mixed precision t·ªët:**
```python
# ƒê√£ c√≥ s·∫µn trong script
with torch.cuda.amp.autocast():
    loss = compute_loss(...)
```

---

## üìä Monitor Training

### **Trong Kaggle notebook:**
```python
# Cell ri√™ng ƒë·ªÉ monitor
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('/kaggle/working/train_val_log_ultimate.csv')
plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.plot(df['epoch'], df['train_loss'], label='Train')
plt.plot(df['epoch'], df['val_loss'], label='Val')
plt.legend()
plt.title('Loss')

plt.subplot(1, 3, 2)
plt.plot(df['epoch'], df['lr'])
plt.title('Learning Rate')

plt.subplot(1, 3, 3)
plt.plot(df['epoch'], df['w_format'], label='Format')
plt.plot(df['epoch'], df['w_answer'], label='Answer')
plt.plot(df['epoch'], df['w_reason'], label='Reason')
plt.legend()
plt.title('Curriculum Weights')

plt.tight_layout()
plt.show()
```

---

## ‚è±Ô∏è ∆Ø·ªõc T√≠nh Th·ªùi Gian

### **T4 GPU (16GB):**
- **1 epoch Ultimate**: ~35 ph√∫t (batch=3)
- **1 epoch Fast**: ~25 ph√∫t (batch=4)
- **100 epochs Ultimate**: ~58 gi·ªù ‚Üí 5 sessions
- **30 epochs Fast**: ~12.5 gi·ªù ‚Üí 2 sessions

### **Chi·∫øn l∆∞·ª£c:**
1. **Session 1**: Ch·∫°y fast mode (30 epochs) ‚Üí c√≥ model kh·∫£ d·ª•ng
2. **Session 2-5**: Ch·∫°y ultimate mode (100 epochs) ‚Üí model t·ªët nh·∫•t

---

## üö® X·ª≠ L√Ω S·ª± C·ªë

### **OOM Error:**
```python
BATCH_SIZE = 2  # Gi·∫£m xu·ªëng
accum_steps = 4  # TƒÉng l√™n ƒë·ªÉ gi·ªØ effective batch = 8
```

### **Session timeout s·∫Øp ƒë·∫øn:**
```python
# Trong cell ri√™ng, check th·ªùi gian
import time
start_time = time.time()

# Sau khi training
elapsed = (time.time() - start_time) / 3600
print(f"Elapsed: {elapsed:.1f} hours")
if elapsed > 11.5:  # G·∫ßn 12h
    print("‚ö†Ô∏è  SESSION S·∫ÆP H·∫æT! DOWNLOAD CHECKPOINT NGAY!")
```

### **Checkpoint b·ªã l·ªói:**
```python
# Ki·ªÉm tra checkpoint tr∆∞·ªõc khi resume
checkpoint = torch.load('/kaggle/input/my-checkpoint/latest_checkpoint.pt')
print(f"Checkpoint epoch: {checkpoint['epoch']}")
print(f"Best val loss: {checkpoint['best_val_loss']}")
print(f"Keys: {checkpoint.keys()}")
```

---

## ‚úÖ Checklist Tr∆∞·ªõc Khi Training

- [ ] B·∫≠t GPU trong Settings (T4 x2 ho·∫∑c P100)
- [ ] Verify paths v·ªõi `verify_kaggle_paths.py`
- [ ] Ch·ªçn script: `train_student_ultimate.py` (t·ªët nh·∫•t) ho·∫∑c `train_student_fast.py` (nhanh)
- [ ] Set `RESUME_FROM = None` cho l·∫ßn ƒë·∫ßu
- [ ] Monitor GPU memory: `!nvidia-smi`
- [ ] Chu·∫©n b·ªã download checkpoint sau 11 gi·ªù

---

## üìÅ Files C·∫ßn Download Sau Training

**B·∫Øt bu·ªôc:**
1. `latest_checkpoint.pt` - Resume training
2. `vqa_student_best_ultimate.pt` - Model t·ªët nh·∫•t
3. `train_val_log_ultimate.csv` - Training history

**T√πy ch·ªçn:**
4. `checkpoint_epoch10.pt`, `checkpoint_epoch20.pt`... - Backup
5. `vqa_student_final_ultimate.pt` - Model cu·ªëi c√πng

**Dung l∆∞·ª£ng:**
- Checkpoint ƒë·∫ßy ƒë·ªß: ~2.5GB
- Model weights only: ~1.7GB
- Log CSV: <1MB
